---
title: "Web-Scrapping Final Project 2020"
subtitle: "Extract Flight Data from Expedia Website - Get Back to Budapest" 
date: 2020-12-13
author: "Yuri Almeida Cunha"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r, echo= FALSE , include = FALSE }

#Load Necessary Packages and Clean the Enviroment 

library(rvest)
library(jsonlite)
library(data.table)
library(httr)
library(tidyverse)
library(utils)
library(survPen)
library(lubridate)
library(knitr)


rm(list=ls())


```


# Introduction

On the following project, It was collected flight information from the Brasilia Airport (BSB) to the Rio de Janeiro (GIG/SDU) in order to be able to decide when and what period of the day to buy a specific set of tickets. With that, the project tries verify what is the correlation between the searched flight prices based on the moment of the day and the remaining days until the trip. 


# Steps to Collect the Data 

## Get to the Website and Define your search terms

Go the the website > https://www.expedia.com.br > once you reach the menu flights. Choose the following configuration:  

From: Brasilia (BSB)  
To: Rio de Janeiro (All Airports)
Date: 07/01/2021  
One Way Flight
1 Passanger  

Once your done, you will be redirected to the page that contains the amount of dedicated flights by company that operates the following path and destination. 

## Retrieve the JSON Link

After selecting the search terms and run the command on the website, a list of flight possibilities will pop up. Open the inspector and check the following URL:  

https://www.expedia.com.br/Flight-Search-Paging?c=c475148f-c55d-4539-b37f-db1b8ed1ba20&is=1&sp=asc&cz=200&cn=0

This is the JSON link that contains most of the necessary information regarding the flights. It's important to take the right one. On the page will find 2 of them, the first one is related to the query parameters used for the search and the second one it's the one that contains the flight information.

## Add the URL and define the summary parameter 

On the webscrapping code there are only two parameters and functions.  

### Functions

The first function is the one the retrieve the summaries based on the filters on the left of the page. The ones selected were the filter by Connection and the one by AirCompany.  
The second function is the one that retrieves the entire flight information dataset, where you can visualize the most important information regarding all flights.

### Parameters

The first parameter is the URL, after copying the URL link, you should replace it on the variable SearchString on the code.  
The second parameter is the one that decides what type of summary statistics you will end up. By default, it was selected the AirCompany one.

PS: Due to a connection limitation it`s important to refresh (copy and paste) the JSON link every 3 hours, you may get disconnected and the following won't retrieve the necessary information.

```{r , echo = TRUE}

SearchString <- "https://www.expedia.com.br/Flight-Search-Paging?c=cf4b1814-5f95-4785-a03b-406cb9315e66&is=1&sp=asc&cz=200&cn=0"
summary_type <- 'Connection'


# Retrieve Flights Summary ------------------------------------------------

#### Doing it to show that we can retrieve also tables from a list instead of unique values 

get_summary_flights <- function(SearchString, summary_type = 'AirCompany'){
  
  
  ##### Define the variables and get the JSON ########
  
  df <- list()
  tbf = data.frame()
  df <- fromJSON(SearchString)
  final_df = data.frame()
  
  if(summary_type == 'Connection'){
  
    ################## FILTER CONNECTION SUMMARY ######################
    
    previous_data <- read.csv("D:/CEU/Web_Scraping/web_scrapping_yuri/Final_Project/Datasets/summary_byconnection.csv") 
    tbf <- df$content$summary$filters$stopFilters  
    tbf$run_time <- Sys.time()
    
    tbf = subset(tbf, select = -c(filterCategory,formattedReferencePrice,formattedPrice))
    
    tbf <- tbf %>% rename( Number_of_Flights = totalCount,
                           Min_Price = priceAsDouble,
                           Qtd_connections = filterName)
    tbf <- separate(data = tbf, into = c("search_date", "search_time"), col = run_time , sep = ' ')
    
    tbf$search_day_period <- with(tbf,  ifelse(hour(hms(search_time)) >= 5 & hour(hms(search_time)) <=11, "morning",
                                        ifelse(hour(hms(search_time))>11 & hour(hms(search_time))<=17, "afternoon", "night")))
    final_df <- dplyr::union(tbf,previous_data)                      
    write.table(final_df , "D:/CEU/Web_Scraping/web_scrapping_yuri/Final_Project/Datasets/summary_byconnection.csv", row.names = FALSE)
    
    return(final_df)
    
  }else{
  
  ################################### SUMMARY AIRLINE FILTERS ##################################################
      
  previous_data <- read.csv("D:/CEU/Web_Scraping/web_scrapping_yuri/Final_Project/Datasets/summary_byaircompany.csv")
  tbf <- df$content$summary$filters$airlineFilters
  tbf$run_time <- Sys.time()
  
  tbf = subset(tbf, select = -c(filterCategory,formattedReferencePrice,formattedPrice))
  
  tbf <- tbf %>% rename( Number_of_Flights = totalCount,
                             Min_Price = priceAsDouble,
                             Air_Line_Company = filterName)
  tbf <- separate(data = tbf, into = c("search_date", "search_time"), col = run_time , sep = ' ')
  tbf$search_day_period <- with(tbf,  ifelse(hour(hms(search_time)) >= 5 & hour(hms(search_time)) <=11, "morning",
                                          ifelse(hour(hms(search_time))>11 & hour(hms(search_time))<=17, "afternoon", "night")))
      
  final_df <- dplyr::union(tbf,previous_data)                      
  write.table(final_df , "D:/CEU/Web_Scraping/web_scrapping_yuri/Final_Project/Datasets/summary_byaircompany.csv", row.names = FALSE)
  
  return(final_df)   
  
  }

}

final_summary <- get_summary_flights(SearchString, summary_type)


# Retrieve_Full_DataSet ---------------------------------------------------


get_flights_table <- function(SearchString){
  
  df_total = data.frame()
  
  df_fromJson <- fromJSON(SearchString)
  
  NestedList <- df_fromJson$content$legs
  
  previous_data <- read.csv("D:/CEU/Web_Scraping/web_scrapping_yuri/Final_Project/Datasets/flights_table.csv")
  
  for (i in 1:length(NestedList)){
    
      t_list <- list()
      t_list[['Fare_Code']] <- NestedList[[i]][['naturalKey']]
      t_list[['Price']] <- NestedList[[i]][['price']][['exactPrice']]
      t_list[['Currency']] <- NestedList[[i]][['price']][['currencyCode']]
      t_list[['AirLineCompany']] <- NestedList[[i]][['carrierSummary']][['airlineName']]
      t_list[['Qtd_Connections']] <- NestedList[[i]][['stops']]
      t_list[['Departure_Airport_City']] <- NestedList[[i]][['departureLocation']][['airportCity']]
      t_list[['Departure_Airport_Code']] <- NestedList[[i]][['departureLocation']][['airportCode']]
      t_list[['Arrival_Airport_City']] <- NestedList[[i]][['arrivalLocation']][['airportCity']]
      t_list[['Arrival_Airport_Code']] <- NestedList[[i]][['arrivalLocation']][['airportCode']]
      t_list[['Departure_Date']] <- NestedList[[i]][['departureTime']][['date']]
      t_list[['Departure_Time']] <- NestedList[[i]][['departureTime']][['time']]
      t_list[['Arrival_Date']] <- NestedList[[i]][['arrivalTime']][['date']]
      t_list[['Arrival_Time']] <- NestedList[[i]][['arrivalTime']][['time']]
      t_list[['Duration_Hours']] <- NestedList[[i]][['duration']][['hours']]
      t_list[['Duration_Minutes']] <- NestedList[[i]][['duration']][['minutes']]
      t_list[['run_time']] <- Sys.time()
      
    df <- data.frame(t_list)
    
    df <- separate(data = df, into = c("search_date", "search_time"), col = run_time , sep = ' ')
    
    df$remaining_days <- as.integer( as.Date(as.character(df$Departure_Date), format="%d/%m/%Y") -
                         as.Date(as.character(df$search_date), format="%Y-%m-%d") )
    
    df$search_day_period <- with(df, ifelse(hour(hms(search_time)) >= 5 & hour(hms(search_time)) <=11, "morning",
                                        ifelse(hour(hms(search_time))>11 & hour(hms(search_time))<=17, "afternoon", "night")))
    
    df_total <- rbind(df_total,df)
    
  }
  
  flights_df <- dplyr::union(df_total,previous_data) 
  
  write.csv(flights_df, 'D:/CEU/Web_Scraping/web_scrapping_yuri/Final_Project/Datasets/flights_table.csv', row.names = FALSE )
  
  return(flights_df)
  
}
 
flights_df <- get_flights_table(SearchString)


```

# The Datasets

Once you finished running the codes you will end up having three/two datasets, one that contains the general information regarding the flights and the other one that contains the summary statistics based on the filter.  

Both of datasets are saved and appended to csv's files to store the previous generated information.  

What is important to mention is the the fact that both of the datasets will have columns that define the precise moment when the information was web scrapped, with the exactly period of the day and the remaining days until the flight departure.  

Please find the flight dataset example bellow: 


```{r, echo = FALSE}

##### Read the file ######

flights_df <- read.csv("D:/CEU/Web_Scraping/web_scrapping_yuri/Final_Project/Datasets/flights_table.csv")

head(flights_df,10)


```


# THE ANALYSIS


After retriving the datasets, itÂ´s time for the analysis. The possible questions are detailed bellow:   

 Is it better to buy flights during the morning, afternoon or at night? Do the prices change?   
 Is there any correlation between the days missing to the flight and their price?  
 
 
 To analyse the following you can you use the following codes:
 
 
 
```{r, echo = FALSE}

## Analysis by Day Period and Air Company

Price_Analysis_by_Day_Period_and_Aircompany <- flights_df %>%
  group_by(search_day_period, AirLineCompany) %>% 
  summarise(
    mean     = mean(Price),
    median   = median(Price),
    std      = sd(Price),
    iq_range = IQR(Price), 
    min      = min(Price),
    max      = max(Price)
)

kable(Price_Analysis_by_Day_Period_and_Aircompany)

```
 

```{r , echo = FALSE}

Price_Analysis_Remaing_days <- flights_df %>%
  group_by(remaining_days) %>% 
  summarise(
    mean     = mean(Price),
    median   = median(Price),
    std      = sd(Price),
    iq_range = IQR(Price), 
    min      = min(Price),
    max      = max(Price)
  )
  

kable(Price_Analysis_Remaing_days)

```


Now that the summary statistics are done, it's time to generate the possible correlations

```{r , echo= FALSE}

ggplot( data = Price_Analysis_Remaing_days, aes( x = remaining_days , y = min ) ) + 
  geom_point( color='blue') +
  geom_smooth( method = lm , color = 'red' )

ggplot( data = Price_Analysis_Remaing_days, aes( x = remaining_days , y = max ) ) + 
  geom_point( color='blue') +
  geom_smooth( method = lm , color = 'red' )

ggplot( data = Price_Analysis_Remaing_days, aes( x = remaining_days , y = mean ) ) + 
  geom_point( color='blue') +
  geom_smooth( method = lm , color = 'red' )


```


# Conclusion


There wasn't enough time to generate a sufficient amount of data in order to really answer those analytics questions. In order to generate the ideas of the graphs, the data was randomly generated to show the structure about what's to be evaluated. 
The project is easy to replicate, you can use different dates and locations, and also and will definitely provide enough information in the future to analyze not only the best ticket buying option, but also how do the air companies regulate their price search data.   






